{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sprint 論文読解入門"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "条件  \n",
    "- 答える際は論文のどの部分からそれが分かるかを書く。\n",
    "- 必要に応じて先行研究（引用されている論文）も探しにいく。最低2つは他の論文を利用して回答すること。\n",
    "- 論文の紹介記事を見ても良い。ただし、答えは論文内に根拠を探すこと。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<メモ>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faster R-CNNと呼ばれるオブジェクト検出システムは、2つのモジュールで構成されています。\n",
    "最初のモジュールは、領域を提案する完全な畳み込みネットワークであり、\n",
    "2番目のモジュールは、提案された領域を使用する高速R-CNN検出器です[ 2 ]。\n",
    "システム全体は、オブジェクト検出用の単一の統合ネットワークです（図  2）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "実はこれはRegion Proposalの時間を除いた場合の比較でFast R-CNNはRegion Proposalの実行時間が支配的になってしまっている。\n",
    "一枚の画像に2.3秒かかるが、そのうち2秒(86%!)がRegionProposalに費やされていた。\n",
    "\n",
    "というのもFast R-CNNではRegion Proposalに従来技術であるSelective Searchを使用しており、そこが速度ボトルネックとなっていた。（コメント有難うございます）\n",
    "Faster R-CNNはRegionProposalもCNN化することで物体検出モデルを全てDNN化し、高速化するのがモチベーションとなっている。\n",
    "\n",
    "またFaster-RCNNはMulti-task lossという学習技術を使っており、RegionProposalモデルも込でモデル全体をend-to-endで学習させることに成功している。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) 物体検出の分野にはどういった手法が存在したか。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <論文引用>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recent advances in object detection are driven by the success of region proposal methods (e.g., [4]) and region-based convolutional neural networks (R-CNNs) [5]. Although region-based CNNs were computationally expensive as originally developed in [5], their cost has been drastically reduced thanks to sharing convolutions across proposals [1, 2]. The latest incarnation, Fast R-CNN [2], achieves near real-time rates using very deep networks [3], when ignoring the time spent on region proposals. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "物体検出における最近の進歩は、領域の提案方法の成功によって駆動される（例えば、[ 4 ]）及び領域ベースの畳み込みニューラルネットワーク（R-CNNs）[ 5 ]。元々に開発された領域ベースCNNsは計算上高価であったが[ 5 ]の提案を横切って共有畳み込みに、それらのコストが大幅に低減されているおかげで、[ 1、2 ]。最新の化身であるFast R-CNN [ 2 ]は、非常に深いネットワーク[ 3 ]を使用してほぼリアルタイムのレートを実現します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 答え"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R-CNNs、CNNs、Fast R-CNN、Selective Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <その他の情報>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e.g., Selective Search [4] object detectors, R- CNN [5], and Fast R-CNN [2]).\n",
    "\n",
    "SPPnet [1] and Fast R-CNN \n",
    "\n",
    "セレクティブサーチ\n",
    "https://staff.fnwi.uva.nl/th.gevers/pub/GeversIJCV2013.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) Fasterとあるが、どういった仕組みで高速化したのか。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <論文引用>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our object detection system, called Faster R-CNN, is composed of two modules. The first module is a deep fully convolutional network that proposes regions, and the second module is the Fast R-CNN detector [2] that uses the proposed regions. The entire system is a single, unified network for object detection (Figure 2). Using the recently popular terminology of neural networks with ‘attention’ [31] mechanisms, the RPN module tells the Fast R-CNN module where to look. In Section 3.1 we introduce the designs and properties of the network for region proposal. In Section 3.2 we develop algorithms for training both modules with features shared."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faster R-CNNと呼ばれるオブジェクト検出システムは、2つのモジュールで構成されています。最初のモジュールは、領域を提案する完全な畳み込みネットワークであり、2番目のモジュールは、提案された領域を使用する高速R-CNN検出器です[ 2 ]。システム全体は、オブジェクト検出用の単一の統合ネットワークです（図  2）。「注意」[ 31 ]メカニズムを備えたニューラルネットワークの最近人気のある用語を使用して、RPNモジュールはFast R-CNNモジュールにどこを見ればよいかを伝えます。セクション  3.1では、地域提案のためのネットワークの設計と特性を紹介します。セクション3.2では  、機能を共有して両方のモジュールをトレーニングするアルゴリズムを開発します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "物体検出にもNNを利用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### その他"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Region Proposal Network (RPN)という物体候補領域を推定してくれるネットワーク+ RoI Poolingにクラス推定を行うことでEnd to Endで学習できるアーキテクチャを提案しました。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) One-Stageの手法とTwo-Stageの手法はどう違うのか。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 論文引用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-Stage Detection vs. Two-Stage Proposal + Detection. The OverFeat paper [9] proposes a detection method that uses regressors and classifiers on sliding windows over convolutional feature maps. OverFeat is a one-stage, class-specific detection pipeline, and ours is a two-stage cascade consisting of class-agnostic proposals and class-specific detections. In OverFeat, the region-wise features come from a sliding window of one aspect ratio over a scale pyramid. These features are used to simultaneously determine the location and category of objects. In RPN, the features are from square (3\n",
    "×\n",
    "3) sliding windows and predict proposals relative to anchors with different scales and aspect ratios. Though both methods use sliding windows, the region proposal task is only the first stage of Faster R-CNN—the downstream Fast R-CNN detector attends to the proposals to refine them. In the second stage of our cascade, the region-wise features are adaptively pooled [1, 2] from proposal boxes that more faithfully cover the features of the regions. We believe these features lead to more accurate detections.\n",
    "To compare the one-stage and two-stage systems, we emulate the OverFeat system (and thus also circumvent other differences of implementation details) by one-stage Fast R-CNN. In this system, the “proposals” are dense sliding windows of 3 scales (128, 256, 512) and 3 aspect ratios (1:1, 1:2, 2:1). Fast R-CNN is trained to predict class-specific scores and regress box locations from these sliding windows. Because the OverFeat system adopts an image pyramid, we also evaluate using convolutional features extracted from 5 scales. We use those 5 scales as in [1, 2].\n",
    "Table X compares the two-stage system and two variants of the one-stage system. Using the ZF model, the one-stage system has an mAP of 53.9%. This is lower than the two-stage system (58.7%) by 4.8%. This experiment justifies the effectiveness of cascaded region proposals and object detection. Similar observations are reported in [2, 39], where replacing SS region proposals with sliding windows leads to \n",
    "∼\n",
    "6% degradation in both papers. We also note that the one-stage system is slower as it has considerably more proposals to process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一段階検出VS。2段階の提案+検出。 OverFeat論文[ 9 ]は、畳み込み特徴マップ上のスライディングウィンドウでリグレッサとクラシファイアを使用する検出方法を提案しています。OverFeatは、一段階、クラス固有の検出パイプライン、そして我々は、二段カスケードクラスにとらわれない提案とクラス固有の検出からなります。OverFeatでは、リージョン単位の機能は、スケールピラミッド上の1つのアスペクト比のスライドウィンドウから取得されます。これらの機能は、オブジェクトの場所とカテゴリを同時に決定するために使用されます。RPNでは、フィーチャは正方形（3 \n",
    "×\n",
    "3）ウィンドウをスライドさせ、異なるスケールとアスペクト比のアンカーに関連する提案を予測します。両方の方法は、スライディングウィンドウを使用しても、領域提案タスクは、R-CNN-下流高速R-CNN検出高速の最初の段階である出席それらをさらに絞り込むことが提案されます。我々のカスケードの第2段階では、領域ごとの特徴が適応的にプールされる[ 1、2 ]より忠実領域の特徴をカバー提案ボックスから。これらの機能は、より正確な検出につながると考えています。\n",
    "\n",
    "1段階システムと2段階システムを比較するために、1 段階高速R-CNN によってOverFeatシステムをエミュレートします（したがって、実装の詳細の他の違いを回避します）。このシステムでは、「提案」は3つのスケール（128、256、512）および3つのアスペクト比（1：1、1：2、2：1）の密なスライドウィンドウです。Fast R-CNNは、これらのスライディングウィンドウからクラス固有のスコアと回帰ボックスの位置を予測するようにトレーニングされています。OverFeatシステムは画像ピラミッドを採用しているため、5つのスケールから抽出された畳み込み機能を使用して評価します。我々は、同様にそれらの5つのスケールを使用し、[ 1、2 ]。\n",
    "\n",
    "表  Xは、2段階システムと1段階システムの2つのバリアントを比較しています。ZFモデルを使用すると、1ステージシステムのmAPは53.9％になります。これは、2段階システム（58.7％）よりも4.8％低いです。この実験は、カスケード領域の提案とオブジェクト検出の有効性を正当化します。同様の観察がで報告されている[ 2、39 ]に窓リードを摺動してSS領域の提案を交換する場合、\n",
    "〜\n",
    "の両方紙6％の分解。また、処理する提案がかなり多いため、1段階システムの方が遅いことにも注意してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### その他"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "two stage detection framework: 最初に領域候補の推定(region proposal)を行い、次にクラスやbounding boxの座標を推定する。\n",
    "one stage detection framework (region proposal free framework): 領域候補の推定と、クラスやbounding boxの座標の推定を1段階で行う。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4) RPNとは何か。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 論文引用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Region Proposal Network (RPN) takes an image (of any size) as input and outputs a set of rectangular object proposals, each with an objectness score.3 We model this process with a fully convolutional network [7], which we describe in this section. Because our ultimate goal is to share computation with a Fast R-CNN object detection network [2], we assume that both nets share a common set of convolutional layers. In our experiments, we investigate the Zeiler and Fergus model [32] (ZF), which has 5 shareable convolutional layers and the Simonyan and Zisserman model [3] (VGG-16), which has 13 shareable convolutional layers.\n",
    "To generate region proposals, we slide a small network over the convolutional feature map output by the last shared convolutional layer. This small network takes as input an \n",
    "n\n",
    "×\n",
    "n\n",
    " spatial window of the input convolutional feature map. Each sliding window is mapped to a lower-dimensional feature (256-d for ZF and 512-d for VGG, with ReLU [33] following). This feature is fed into two sibling fully-connected layers—a box-regression layer (reg) and a box-classification layer (cls). We use \n",
    "n\n",
    "=\n",
    "3\n",
    " in this paper, noting that the effective receptive field on the input image is large (171 and 228 pixels for ZF and VGG, respectively). This mini-network is illustrated at a single position in Figure 3 (left). Note that because the mini-network operates in a sliding-window fashion, the fully-connected layers are shared across all spatial locations. This architecture is naturally implemented with an \n",
    "n\n",
    "×\n",
    "n\n",
    "convolutional layer followed by two sibling \n",
    "1\n",
    "×\n",
    "1\n",
    " convolutional layers (for reg and cls, respectively)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "領域提案ネットワーク（RPN）は、入力として（任意のサイズの）画像を受け取り、それぞれがオブジェクト性スコアを持つ一連の長方形のオブジェクト提案を出力します。3このセクションで説明 する完全な畳み込みネットワーク[ 7 ]を使用して、このプロセスをモデル化します。最終的な目標は計算を高速R-CNNオブジェクト検出ネットワーク[ 2 ]と共有することであるため、両方のネットが畳み込み層の共通セットを共有すると仮定します。実験では、5つの共有可能な畳み込み層を持つZeiler and Fergusモデル[ 32 ]（ZF）と、Simonyan and Zissermanモデル[ 3 ]を調査します。 （VGG-16）、13の共有可能な畳み込み層があります。\n",
    "\n",
    "地域の提案を生成するために、最後の共有畳み込み層によって出力された畳み込み特徴マップ上に小さなネットワークをスライドさせます。この小さなネットワークは、入力として、入力畳み込み特徴マップの\n",
    "n \n",
    "× \n",
    "n\n",
    "空間ウィンドウを取ります。各スライディングウィンドウは、低次元のフィーチャにマッピングされます（ZFの場合は256-d、VGGの場合は512-d、ReLU [ 33 ]が後に続きます）。この機能は、完全に接続された2つの兄弟層（ボックス回帰層（reg）とボックス分類層（cls））に供給されます。\n",
    "n \n",
    "= \n",
    "3\n",
    "を使用します\n",
    "このホワイトペーパーでは、入力画像の有効な受容野が大きいことに注意してください（ZFおよびVGGでそれぞれ171および228ピクセル）。このミニネットワークは、図3（左）の単一の位置に示されてい  ます。ミニネットワークはスライディングウィンドウ方式で動作するため、完全に接続されたレイヤーはすべての空間ロケーションで共有されることに注意してください。このアーキテクチャは、\n",
    "n \n",
    "× \n",
    "n\n",
    "畳み込み層とそれに続く2つの兄弟の\n",
    "1 \n",
    "× \n",
    "1\n",
    "畳み込み層（それぞれregとclsの場合）で自然に実装されます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### その他"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本文から\n",
    "入力として（任意のサイズの）画像を受け取り、\n",
    "それぞれがオブジェクト性スコアを持つ一連の長方形のオブジェクト提案を出力します  \n",
    "RPNは、物体候補を出力するために(1)物体かどうかを表すスコア(図中cls layer)と\n",
    "(2)物体の領域(図中reg layer)の２つを同時に出力するように設計されています。\n",
    "画像全体のfeature mapsから予め決められたk個の固定枠(Anchor)を用いて特徴を抽出し、\n",
    "RPNの入力とすることで、各場所において物体候補とすべきかどうかを推定します。\n",
    "\n",
    "物体候補として推定された出力枠(reg layer)の範囲を、\n",
    "Fast R-CNN同様にRoI Poolingしクラス識別用のネットワークの入力とすることで最終的な物体検出を実現します。\n",
    "物体候補検出がDeep化されたことで、既存手法(Selective Search)[7]よりも物体候補が高精度化&候補数が少なくなり、 \n",
    "GPU上で5fpsの実行速度(VGGのネットワークを利用)を達成しました。 また、識別精度もFast-RCNNより高精度化しています。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Region Propsal Network(RPN)はCNNの特徴マップ（つまりResnet等のCNN層出力）を入力。\n",
    "\n",
    "ある領域が物体か背景か(objectness)およびアンカーの位置の補正データ(corrdinates)を出力する。objectnessは0-1の値で1に近いほど物体である確証が高い。corrdinatesはBindingBoxの四角の座標について補正する量を出力する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (5) RoIプーリングとは何か。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 論文引用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nevertheless, our method achieves bounding-box\n",
    "regression by a different manner from previous RoIbased (Region of Interest) methods [1], [2]. In [1],\n",
    "[2], bounding-box regression is performed on features\n",
    "pooled from arbitrarily sized RoIs, and the regression\n",
    "weights are shared by all region sizes. In our formulation, the features used for regression are of the same\n",
    "spatial size (3 × 3) on the feature maps. To account\n",
    "for varying sizes, a set of k bounding-box regressors\n",
    "are learned. Each regressor is responsible for one scale\n",
    "and one aspect ratio, and the k regressors do not share\n",
    "weights. As such, it is still possible to predict boxes of\n",
    "various sizes even though the features are of a fixed\n",
    "size/scale, thanks to the design of anchors.  \n",
    "As discussed\n",
    "above, the bounding boxes predicted by RPN are\n",
    "also functions of the input. The RoI pooling layer\n",
    "[2] in Fast R-CNN accepts the convolutional features\n",
    "and also the predicted bounding boxes as input, so\n",
    "a theoretically valid backpropagation solver should\n",
    "also involve gradients w.r.t. the box coordinates. These\n",
    "gradients are ignored in the above approximate joint\n",
    "training. In a non-approximate joint training solution,\n",
    "we need an RoI pooling layer that is differentiable\n",
    "w.r.t. the box coordinates. This is a nontrivial problem\n",
    "and a solution can be given by an “RoI warping” layer\n",
    "as developed in [15], which is beyond the scope of this\n",
    "paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "私たちの方法は境界ボックスを達成します\n",
    "以前のRoIbased（関心のある地域）メソッド[1]、[2]とは異なる方法による回帰。 1で]、\n",
    "[2]、境界ボックス回帰はフィーチャに対して実行されます\n",
    "任意のサイズのRoIからプールされ、回帰\n",
    "重みはすべての領域サイズで共有されます。私たちの定式化では、回帰に使用される特徴は同じです\n",
    "機能マップ上の空間サイズ（3×3）。アカウントへ\n",
    "さまざまなサイズの場合、k個の境界ボックスリグレッサのセット\n",
    "学びます。各リグレッサーは1つのスケールを担当します\n",
    "そして1つのアスペクト比、およびk個のリグレッサは共有しません\n",
    "重み。そのため、次のボックスを予測することは依然として可能です。\n",
    "機能が固定されていても、さまざまなサイズ\n",
    "アンカーの設計のおかげでサイズ/スケール。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### その他"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RoI Poolingでは、ある程度畳み込み処理を行ったfeature mapから、region proposalにあたる部分領域をうまく\n",
    "「固定サイズのfeature map」として抽出します。\n",
    "https://qiita.com/yu4u/items/5cbe9db166a5d72f9eb8\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (6) Anchorのサイズはどうするのが適切か。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 論文引用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The anchor boxes that cross image boundaries need\n",
    "to be handled with care. During training, we ignore\n",
    "all cross-boundary anchors so they do not contribute\n",
    "to the loss. For a typical 1000 × 600 image, there\n",
    "will be roughly 20000 (≈ 60 × 40 × 9) anchors in\n",
    "total. With the cross-boundary anchors ignored, there\n",
    "are about 6000 anchors per image for training. If the\n",
    "boundary-crossing outliers are not ignored in training,\n",
    "they introduce large, difficult to correct error terms in\n",
    "the objective, and training does not converge. During\n",
    "testing, however, we still apply the fully convolutional\n",
    "RPN to the entire image. This may generate crossboundary proposal boxes, which we clip to the image\n",
    "boundary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "画像の境界を越えるアンカーボックスが必要\n",
    "注意して取り扱ってください。トレーニング中、無視します\n",
    "すべての境界を越えたアンカーなので、貢献しません\n",
    "損失に。典型的な1000×600画像の場合、\n",
    "約20000（≈60×40×9）アンカーになります\n",
    "合計。境界を越えたアンカーを無視して、そこに\n",
    "トレーニング用の画像ごとに約6000個のアンカーがあります。もし\n",
    "境界を越える外れ値はトレーニングでは無視されません。\n",
    "修正が困難な大きなエラー用語を導入します\n",
    "目的、およびトレーニングは収束しません。中に\n",
    "ただし、テストでは、完全な畳み込みを適用します\n",
    "画像全体に対するRPN。これにより、境界を越えた提案ボックスが生成され、画像にクリップされます。\n",
    "境界。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (7) 何というデータセットを使い、先行研究に比べどういった指標値が得られているか。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 論文引用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We present more results on the Microsoft COCO\n",
    "object detection dataset [12]. This dataset involves 80\n",
    "object categories. We experiment with the 80k images\n",
    "on the training set, 40k images on the validation set,\n",
    "and 20k images on the test-dev set. We evaluate the\n",
    "mAP averaged for IoU ∈ [0.5 : 0.05 : 0.95] (COCO’s\n",
    "standard metric, simply denoted as mAP@[.5, .95])\n",
    "and mAP@0.5 (PASCAL VOC’s metric)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Microsoft COCOオブジェクト検出データセット[ 12 ]でより多くの結果を提示します。このデータセットには80のオブジェクトカテゴリが含まれます。トレーニングセットの80k画像、検証セットの40k画像、test-devセットの20k画像で実験します。私たちは、マップがIOUのための平均評価\n",
    "∈ \n",
    "[ \n",
    "0.5 \n",
    "：\n",
    "0.05 \n",
    "：\n",
    "0.95 \n",
    "]\n",
    "（COCOの標準メトリック、単にと表記MAP @ [0.5、0.95]）とmAP@0.5 （PASCAL VOCの測定基準）。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Table 11 we first report the results of the Fast\n",
    "R-CNN system [2] using the implementation in this\n",
    "paper. Our Fast R-CNN baseline has 39.3% mAP@0.5\n",
    "on the test-dev set, higher than that reported in [2].\n",
    "We conjecture that the reason for this gap is mainly\n",
    "due to the definition of the negative samples and also\n",
    "the changes of the mini-batch sizes. We also note that\n",
    "the mAP@[.5, .95] is just comparable.\n",
    "Next we evaluate our Faster R-CNN system. Using\n",
    "the COCO training set to train, Faster R-CNN has\n",
    "42.1% mAP@0.5 and 21.5% mAP@[.5, .95] on the\n",
    "COCO test-dev set. This is 2.8% higher for mAP@0.5\n",
    "and 2.2% higher for mAP@[.5, .95] than the Fast RCNN counterpart under the same protocol (Table 11).\n",
    "This indicates that RPN performs excellent for improving the localization accuracy at higher IoU thresholds. Using the COCO trainval set to train, Faster RCNN has 42.7% mAP@0.5 and 21.9% mAP@[.5, .95] on\n",
    "the COCO test-dev set. Figure 6 shows some results\n",
    "on the MS COCO test-dev set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "表  XIでは、最初に、このペーパーの実装を使用したFast R-CNNシステム[ 2 ]の結果を報告します。Fast R-CNNベースラインのtest-devセットのmAP@0.5 は39.3％で、[ 2 ]で報告されたものよりも高くなっています。このギャップの原因は、主にネガティブサンプルの定義とミニバッチサイズの変更によるものと推測します。また、mAP @ [.5、.95]は単なる比較にすぎないことに注意してください。\n",
    "\n",
    "次に、より高速なR-CNNシステムを評価します。COCOトレーニングセットを使用してトレーニングを行うと、Faster R-CNNはCOCOテスト開発セットで42.1％mAP@0.5 および21.5％mAP @ [.5、.95]になります。これは2.8％高いmAP@0.5 および2.2％より高いのためのマップ@ [0.5、0.95]同じプロトコル（表の下の高速R-CNN対応物より  XI）。これは、RPNが高いIoUしきい値でのローカライズの精度を向上させるために優れたパフォーマンスを発揮することを示しています。COCO trainvalセットを使用してトレーニングを行うと、Faster R-CNNはCOCO test-devセットで42.7％mAP@0.5 および21.9％mAP @ [.5、.95]になります。図  6は、MS COCO test-devセットのいくつかの結果を示しています。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
